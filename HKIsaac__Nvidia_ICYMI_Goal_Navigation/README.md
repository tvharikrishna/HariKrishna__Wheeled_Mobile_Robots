<!------ PROJECT TITLE ------>
<p align="center">
    <img src="readme_data/project_title.png" alt="title" width="1500"/>
</p> <br> <br>

<!------ WHAT ------>
<p align="center">
    <img src="readme_data/what.png" alt="what" width="600"/>
</p>

<p align="center"><h1>ðŸŽ€ Essence of the Project</h1></p>
<p align='justify'>
The 'NVIDIA ICYMI Point Clicked Goal Navigation' project showcases the capabilities of the NVIDIA ICYMI robot in a simulated environment using the NVIDIA Isaac SIM platform. This project focuses on enhancing the robot's navigation abilities, allowing it to move towards a user-defined point within the environment. Utilizing advanced sensors and real-time processing, the robot efficiently navigates while avoiding obstacles, mapping its surroundings, and providing depth estimations and visual feedback through integrated RGB and depth cameras.
</p>

<p align="center">
  <a href="https://www.youtube.com/watch?v=3CQRbCDHU30">
    <img src="https://img.shields.io/badge/My Project Video-Nvidia ICYMI Goal Navigation-blue" alt="Video" width="470" height="38"/>
  </a>
</p> <hr> <br> <br> 

<!------ WHY ------>
<p align="center">
    <img src="readme_data/why.png" alt="What the project accomplishes" width="600"/>
</p>

<p align="center"><h1>ðŸŽ¯ Project Vision</h1></p>
<p style="text-align: justify;">
The vision behind the NVIDIA ICYMI Point Clicked Goal Navigation project is to demonstrate a scalable and adaptable framework for autonomous robot navigation in dynamically changing environments. The goal is to push the boundaries of what autonomous systems can achieve in terms of precision navigation and environmental interaction, thereby paving the way for broader applications in industrial automation, logistics, and service sectors where advanced navigation is crucial.
</p> <hr> <br> <br> 


<!------ HOW ------>
<p align="center">
    <img src="readme_data/how.png" alt="How we implemented the project" width="600"/>
</p>

<p align="center"><h1>ðŸª“Project Implementation</h1></p>
<p><h2>ðŸ’  Software Design & Tools </h2></p>
<p align='justify'>
The implementation of this project is rooted in a powerful and diverse toolkit, embracing both Ubuntu and Linux for robust operating system support, with Python as the backbone for scripting and automation. The project is built upon the ROS ecosystem and powered by the advanced capabilities of NVIDIA Isaac SIM for realistic simulation and testing.
</p>

<p>
<img src="https://img.shields.io/badge/Ubuntu-E95420.svg?&style=flat-square&logo=ubuntu&logoColor=white" alt="Ubuntu" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/Linux-FCC624.svg?&style=flat-square&logo=linux&logoColor=black" alt="Linux" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/Python-3776AB.svg?&style=flat-square&logo=python&logoColor=white" alt="Python" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/ROS1-22314E.svg?&style=flat-square&logo=ros&logoColor=white" alt="ROS" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/VS%20Code-007ACC.svg?&style=flat-square&logo=visual-studio-code&logoColor=white" alt="VS Code" style="height: 30px;"/> &nbsp;
<img src="https://img.shields.io/badge/NVIDIA%20Isaac%20SIM-76B900.svg?&style=flat-square&logo=nvidia&logoColor=white" alt="NVIDIA Isaac SIM" style="height: 30px;"/> &nbsp;
</p> <br> 

<!------ Technical Terms ------>
<p align="center"><h2>ðŸ’  Project Technical Terms & Concepts </h2></p>
<p align="center"><h3>â–¸ What is Nvidia's ICYMI Robot?</h3></p>
<p style="text-align: justify;">
Nvidia's ICYMI (In Case You Missed It) Robot is a versatile robotic platform designed for research and development in the fields of robotics and artificial intelligence. It leverages Nvidia's cutting-edge hardware and software technologies to enable advanced functionalities like autonomous navigation, object recognition, and real-time environmental interaction.
</p> <br>

<p align="center"><h3>â–¸ What is Point Clicked Goal Navigation in ROS?</h3></p>
<p style="text-align: justify;">
Point Clicked Goal Navigation in ROS refers to a navigation technique where a robot moves to a target location selected by the user in a graphical interface. This is typically achieved by processing the click input to determine the target position in the robot's coordinate frame and then generating a path to that position while avoiding obstacles.
</p> <br>

<p align="center"><h3>â–¸ What is an Occupancy grid?</h3></p>
<p style="text-align: justify;">
An occupancy grid is a data structure used in robotics and autonomous vehicle navigation to represent the environment. It consists of a grid that divides the space into cells, where each cell holds a value indicating whether the area is occupied, free, or unknown. This helps the robot in path planning and navigation by providing a detailed map of the surroundings.
</p> <br>

<p align="center"><h3>â–¸ What is SLAM Gmapping?</h3></p>
<p style="text-align: justify;">
SLAM Gmapping is a technique used to create a map of an environment while simultaneously tracking the location of the robot within that map. It uses a particle filter-based approach to solve the SLAM (Simultaneous Localization and Mapping) problem, typically employing laser range finders to perceive the environment.
</p> <br>

<p align="center"><h3>â–¸ What is a Global Path Planner?</h3></p>
<p style="text-align: justify;">
A Global Path Planner is a component of robotic navigation systems that generates a feasible route from the robot's current position to a specified goal location over the entire map. It considers the known environment and calculates the most efficient path, often using algorithms like A* or Dijkstraâ€™s algorithm.
</p> <br>

<p align="center"><h3>â–¸ What is a Local Path Planner?</h3></p>
<p style="text-align: justify;">
A Local Path Planner operates within the robot's immediate environment to dynamically adjust the robot's path in response to unforeseen obstacles or changes in the environment. It ensures the robot's safety by making quick decisions about path alterations to avoid collisions and optimize movement.
</p> <br>

<p align="center"><h3>â–¸ What is the difference between RGB Image and Depth Image?</h3></p>
<p style="text-align: justify;">
The difference between RGB Image and Depth Image lies in the type of information each provides. An RGB image captures the visual appearance of the environment in color, similar to what the human eye sees. A Depth image, on the other hand, provides the distance of surfaces from the camera's perspective, offering a 3D perception by measuring how far objects are from the point of view.
</p> <br>

<!------ Deployment and Testing ------>
<p align="center"><h2>ðŸ’  Deployment and Testing</h2></p> <br>
<p align="center">
    <img src="readme_data/project_obs1.png" alt="Deployment and Testing Images" width="1500"/>
</p> <br>

<p align="center">
    <img src="readme_data/project_obs2.png" alt="Deployment and Testing Images" width="1500"/>
</p> <br>

<p align="center">
    <img src="readme_data/project_obs3.png" alt="Deployment and Testing Images" width="1500"/>
</p> <br>

<p align="center">
    <img src="readme_data/project_obs4.png" alt="Deployment and Testing Images" width="1500"/>
</p> <br>

<p align="center">
    <img src="readme_data/project_obs5.png" alt="Deployment and Testing Images" width="1500"/>
</p> <hr> <br> <br>

<!------ End Image ------>
<p align="center">
    <img src="readme_data/hk_quote.png" alt="endquote" width="1500"/>
</p>
