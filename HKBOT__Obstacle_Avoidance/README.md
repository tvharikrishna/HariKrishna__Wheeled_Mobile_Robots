<!------ PROJECT TITLE ------>
<p align="center">
    <img src="readme_data/project_title.png" alt="title" width="1111"/>
</p> <br> <br>

<!------ WHAT ------>
<p align="center">
    <img src="readme_data/what.png" alt="what" width="600"/>
</p> 

<p align="center"><h1>ðŸŽ€ Essence of the Project</h1></p>
<p align='justify'>
This project introduces the HK Bot, a custom-made mobile robot designed for autonomous navigation. It uses YdLidar technology for accurate environmental scanning and is built on the Robot Operating System (ROS), providing a robust and flexible platform for programming robots. Additionally, the robot's spatial data and sensor inputs are displayed using rviz, ROS's 3D visualization tool, which helps in plotting 2D LiDAR points to thoroughly understand the robot's perception and navigation capabilities.
</p>

<p align="center">
  <a href="https://www.youtube.com/watch?v=Xat4bXAMsAk&list=PL0phN1wjvpsafQdjjdUhWytGB8bZFhxXy&index=9">
    <img src="https://img.shields.io/badge/My Project Video-Obstacle Evader Robot-blue" alt="Video" width="380" height="35"/>
  </a>
</p> <hr> <br> <br>

<!------ WHY ------>
<p align="center">
    <img src="readme_data/why.png" alt="Why" width="600"/>
</p>

<p align="center"><h1>ðŸŽ¯ Project Vision</h1></p>
<p style="text-align: justify;">
This project focuses on advancing obstacle avoidance technology, essential for the safety and efficiency of autonomous mobile robots and vehicles. It aims to enhance autonomous navigation in complex environments by incorporating real-time decision-making capabilities. Obstacle avoidance is crucial for preventing collisions, particularly in unpredictable settings. This technology allows autonomous machines to adapt quickly to new obstacles, improving the reliability and effectiveness of services like automated delivery or emergency aid. Additionally, it reduces the need for human oversight, advancing autonomous driving and contributing to safer roads.
</p> <hr> <br> <br>

<!------ HOW ------>
<p align="center">
    <img src="readme_data/how.png" alt="How" width="600"/>
</p>

<p align="center"><h1>ðŸª“Project Implementation</h1></p>
<p><h2>ðŸ’  Software Design & Tools </h2></p>
<p align='justify'>
The project is developed using a robust and versatile tech stack, comprising Ubuntu and Linux for the operating systems, Python as the primary programming language, and utilizing essential tools like SSH, PuTTY, and VNC Viewer for secure remote connections. Development and simulation are enhanced with the ROS ecosystem, including RViz for visualization.
</p>

<p>
<img src="https://img.shields.io/badge/Ubuntu-E95420.svg?&style=flat-square&logo=ubuntu&logoColor=white" alt="Ubuntu" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/Linux-FCC624.svg?&style=flat-square&logo=linux&logoColor=black" alt="Linux" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/Python-3776AB.svg?&style=flat-square&logo=python&logoColor=white" alt="Python" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/SSH-4D4D4D.svg?&style=flat-square&logo=windows-terminal&logoColor=white" alt="SSH" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/PuTTY-007ACC.svg?&style=flat-square&logo=windows-terminal&logoColor=white" alt="PuTTY" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/VNC%20Viewer-ED1C24.svg?&style=flat-square&logo=CodeSandbox&logoColor=white" alt="VNC Viewer" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/ROS-22314E.svg?&style=flat-square&logo=ros&logoColor=white" alt="ROS" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/RVIZ-000000.svg?&style=flat-square&logo=ros&logoColor=white" alt="RViz" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/Numpy-013243.svg?&style=flat-square&logo=numpy&logoColor=white" alt="Numpy" style="height: 25px;"/> &nbsp;
<img src="https://img.shields.io/badge/VS%20Code-007ACC.svg?&style=flat-square&logo=visual-studio-code&logoColor=white" alt="VS Code" style="height: 25px;"/> &nbsp;
</p> <br>

<!------ Technical Terms ------>
<p align="center"><h2>ðŸ’  Project Technical Terms & Concepts</h2></p>
<p align="center"><h3>â–¸ What is YdLidar:</h3></p>
<p style="text-align: justify;">
YdLiDAR is a type of Light Detection and Ranging (LiDAR) sensor designed for use in robotics and automation. LiDAR sensors measure distances by illuminating targets with laser light and measuring the reflection with a sensor. YdLiDAR sensors are known for their cost-effectiveness and compact size, making them suitable for applications like obstacle avoidance, area mapping, and robot navigation where precise distance measurements and environmental awareness are crucial.
</p> <br>

<p align="center">
  <img src="readme_data/ydlidar.png" alt="Technical Term Image" width="220"/>
</p>

<p align="center"><h3>â–¸ How Does Lidar Work:</h3></p>
<p style="text-align: justify;">
TOF (Time of Flight) LiDAR is a technology that calculates the distance to a target by measuring the travel time of a light pulse. The process begins when a laser transmitter emits a beam of modulated light. This light travels to the target, bounces back, and is then detected by the laser receiver in the LiDAR system. The core of TOF LiDAR's functionality lies in its ability to discern the phase difference between the light sent out and the light that returns. By accurately measuring this phase shift, the system can calculate the precise distance to the object with remarkable accuracy, making TOF LiDAR an essential tool for detailed spatial measurements and mapping.
</p>

<div align="center">
  <table>
    <tr>
      <td rowspan="2"><img src="readme_data/lidarscan.gif" alt="Technical Term Image" width="350"/></td>
      <td><img src="readme_data/tof_1.png" alt="Technical Term Image" width="525"/></td>
    </tr>
    <tr>
      <td><img src="readme_data/tof_2.jpg" alt="Technical Term Image" width="525"/></td>
    </tr>
  </table>
</div>

<!------ Deployment and Testing ------>
<p align="center"><h2>ðŸ’  Deployment and Testing</h2></p>
<p style="text-align: justify;">
â–¸ During the deployment and testing phase, the mobile robot was equipped with the aforementioned tools and sensors, which facilitated the implementation of the project. By integrating these technologies, the robot was able to perform autonomous navigation and obstacle avoidance. 
    
â–¸ The real-world performance of the robot was validated by plotting a 2D scan of its environment, which was then visualized in the rviz tool. This visualization provided a clear and detailed representation of how the robot perceives and interacts with its surroundings, demonstrating the effectiveness of the technologies in practical scenarios.
</p>

<p align="center">
  <img src="readme_data/project_observation_1.png" alt="Deployment and Testing Image" width="1111"/>
</p>

<!------ End Image ------>
<p align="center">
  <img src="readme_data/project_observation_2.png" alt="Deployment and Testing Image" width="1111"/>
</p> <hr>

<!------ End Image ------>
<p align="center">
    <img src="readme_data/HKbot_endquote.png" alt="endquote" width="1500"/>
</p>
